{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install vizdoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vizdoom: RL platform, runs very quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd github & git clone https://github.com/mwydmuch/ViZDoom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import game env\n",
    "from vizdoom import *\n",
    "# for random action\n",
    "import random\n",
    "# for sleeping\n",
    "import time\n",
    "# create action space for random actions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup game\n",
    "game = DoomGame()\n",
    "game.load_config('github/ViZDoom/scenarios/deadly_corridor.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple action space without double inputs\n",
    "actions = np.identity(7, dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    game.new_episode()\n",
    "    while not game.is_episode_finished():\n",
    "        state = game.get_state()\n",
    "        img = state.screen_buffer\n",
    "        info = state.game_variables\n",
    "        reward = game.make_action(random.choice(actions)) # frame skip 4 -> get reward after 4 frames\n",
    "        print(reward)\n",
    "        time.sleep(1/50)\n",
    "    print('Total results:', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wrap in Gym wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base class\n",
    "from gym import Env\n",
    "# import spaces\n",
    "from gym.spaces import Discrete, Box # Discrete is like range(), Box is like array\n",
    "# import opencv\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vizdoom env\n",
    "class ViZDoomGym(Env):\n",
    "    def __init__(self, render=False, config='github/ViZDoom/scenarios/deadly_corridor_s1.cfg'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_nr = 7\n",
    "\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "\n",
    "        self.game.set_window_visible(render)\n",
    "\n",
    "        self.game.init()\n",
    "\n",
    "        self.observation_space = Box(0, 255, shape=(100, 160, 1), dtype='uint8')\n",
    "        self.action_space = Discrete(self.action_nr)\n",
    "\n",
    "        self.damage_taken = 0\n",
    "        self.hit_count = 0\n",
    "        self.ammo = 52\n",
    "\n",
    "    def step(self, action):\n",
    "        actions = np.identity(self.action_nr, dtype='uint8')\n",
    "        movement_reward = self.game.make_action(actions[action])\n",
    "\n",
    "        reward = 0\n",
    "        if self.game.get_state(): # interesting line\n",
    "            state = self.game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            img = self.grayscale(img)\n",
    "            \n",
    "            # reward shaping\n",
    "            game_variables = state.game_variables\n",
    "            health, damage_taken, hit_count, ammo = game_variables\n",
    "            \n",
    "            damage_taken_delta = - damage_taken + self.damage_taken # when hit gives negaitve value\n",
    "            self.damage_taken = damage_taken\n",
    "            hit_count_delta = hit_count - self.hit_count # when kills gives positive value\n",
    "            self.hit_count = hit_count\n",
    "            ammo_delta = ammo - self.ammo # when shoot give negative value\n",
    "            self.ammo = ammo\n",
    "            \n",
    "            reward = movement_reward + damage_taken_delta * 10 + hit_count_delta * 200 + ammo_delta * 5\n",
    "            \n",
    "            info = {}\n",
    "\n",
    "        else:\n",
    "            img = np.zeros(self.observation_space.shape)\n",
    "            info = {}\n",
    "\n",
    "        done = self.game.is_episode_finished()\n",
    "\n",
    "        return img, reward, done, info\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.game.close()\n",
    "        super().__del__()\n",
    "\n",
    "    def render():\n",
    "        pass # handled by vizdoom itself\n",
    "\n",
    "    def grayscale(self, observation):\n",
    "        img = cv.cvtColor(np.moveaxis(observation, 0, -1), cv.COLOR_BGR2GRAY)\n",
    "        # scale image down for performance\n",
    "        img = cv.resize(img, (160, 100), interpolation=cv.INTER_CUBIC)\n",
    "        img = np.reshape(img, (100, 160, 1)) # add one dimension\n",
    "        return img\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.new_episode()\n",
    "        img = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ViZDoomGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phiga\\miniconda3\\envs\\reinforcement\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# verify environment\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "img = env.step(1)[0]\n",
    "plt.imshow(img, cmap='gray')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './doom/train_corridor' # for model weights\n",
    "LOG_DIR = './doom/log_corridor' # for tf logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=33000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the model with curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ViZDoomGym.__del__ at 0x000002C64EB69090>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\phiga\\AppData\\Local\\Temp\\ipykernel_20460\\2200486960.py\", line 60, in __del__\n",
      "AttributeError: 'super' object has no attribute '__del__'\n"
     ]
    }
   ],
   "source": [
    "env = ViZDoomGym(config='github/ViZDoom/scenarios/deadly_corridor_s1.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=4096)\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./doom/log_corridor\\PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.27e+03 |\n",
      "|    ep_rew_mean     | 63.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 68       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 811         |\n",
      "|    ep_rew_mean          | 31.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035039023 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | -7.15e-07   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 541         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.0018      |\n",
      "|    value_loss           | 1.14e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\phiga\\OneDrive\\Zwischenspeicher\\Projects maker\\Programming\\reinforcement learning\\doom_corridor.ipynb Zelle 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/phiga/OneDrive/Zwischenspeicher/Projects%20maker/Programming/reinforcement%20learning/doom_corridor.ipynb#ch0000027?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[1;32mc:\\Users\\phiga\\miniconda3\\envs\\reinforcement\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:310\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    298\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    299\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    311\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    312\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    313\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    314\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    315\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    316\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    317\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    318\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    319\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    320\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\phiga\\miniconda3\\envs\\reinforcement\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:247\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    243\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    245\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 247\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phiga\\miniconda3\\envs\\reinforcement\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:175\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox):\n\u001b[0;32m    173\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> 175\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    179\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phiga\\miniconda3\\envs\\reinforcement\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\phiga\\miniconda3\\envs\\reinforcement\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[0;32m     97\u001b[0m     \u001b[39m# Transpose the terminal observations\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m idx, done \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dones):\n",
      "File \u001b[1;32mc:\\Users\\phiga\\miniconda3\\envs\\reinforcement\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\phiga\\miniconda3\\envs\\reinforcement\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "\u001b[1;32mc:\\Users\\phiga\\OneDrive\\Zwischenspeicher\\Projects maker\\Programming\\reinforcement learning\\doom_corridor.ipynb Zelle 28\u001b[0m in \u001b[0;36mViZDoomGym.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phiga/OneDrive/Zwischenspeicher/Projects%20maker/Programming/reinforcement%20learning/doom_corridor.ipynb#ch0000027?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phiga/OneDrive/Zwischenspeicher/Projects%20maker/Programming/reinforcement%20learning/doom_corridor.ipynb#ch0000027?line=22'>23</a>\u001b[0m     actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39midentity(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_nr, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/phiga/OneDrive/Zwischenspeicher/Projects%20maker/Programming/reinforcement%20learning/doom_corridor.ipynb#ch0000027?line=23'>24</a>\u001b[0m     movement_reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mmake_action(actions[action])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phiga/OneDrive/Zwischenspeicher/Projects%20maker/Programming/reinforcement%20learning/doom_corridor.ipynb#ch0000027?line=25'>26</a>\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/phiga/OneDrive/Zwischenspeicher/Projects%20maker/Programming/reinforcement%20learning/doom_corridor.ipynb#ch0000027?line=26'>27</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_state(): \u001b[39m# interesting line\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(CHECKPOINT_DIR + '/best_model_99000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ViZDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(5):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        time.sleep(1/50)\n",
    "        total_reward += reward\n",
    "    print(f'Reward of episode {episode+1} is {total_reward}')\n",
    "    time.sleep(2)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not perfoect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('reinforcement')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4984e7ba3fca0e09acc1abfa4e61dc59df22bf9db11b60a1a72c49fd6cc05221"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
